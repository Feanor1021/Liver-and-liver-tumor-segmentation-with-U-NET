{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sticky-essay",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "* pathlib for easy path handling\n",
    "* HTML for visualizing volume videos\n",
    "* torchio for dataset creation\n",
    "* torch for DataLoaders, optimizer and loss\n",
    "* pytorch-lightning for training\n",
    "* numpy for masking\n",
    "* matplotlib for visualization\n",
    "* Our 3D model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tropical-laugh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torchio as tio\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05174b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-canon",
   "metadata": {},
   "source": [
    "## Dataset Creation\n",
    "We can loop over all available scans and add them to the subject list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "square-witness",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_img_to_label_path(path):\n",
    "    parts = list(path.parts)\n",
    "    parts[parts.index(\"imagesTr\")] = \"labelsTr\"\n",
    "    return Path(*parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rapid-plastic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = Path(\"Liver/imagesTr/\")\n",
    "subject_paths = list(path.glob(\"liver*\"))# Choose a subject\n",
    "subjects = []\n",
    "\n",
    "for subject_path in subject_paths:\n",
    "    label_path = change_img_to_label_path(subject_path)\n",
    "    subject = tio.Subject({\"CT\":tio.ScalarImage(subject_path), \"Label\":tio.LabelMap(label_path)})\n",
    "    subjects.append(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f445f7",
   "metadata": {},
   "source": [
    "To make sure the all orientations are same we will look them all by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc56abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    assert subject[\"CT\"].orientation == (\"R\", \"A\", \"S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-bread",
   "metadata": {},
   "source": [
    "we have dataset of different sizes and our U-NET model needs same sizes for all images. Regarding the processing, we use the **CropOrPad** functionality which crops or pads all images and masks to the same shape.\n",
    "\n",
    "We use ($256 \\times 256 \\times 200$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hearing-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = tio.Compose([\n",
    "            tio.CropOrPad((256, 256, 200)),\n",
    "            tio.RescaleIntensity((-1, 1))\n",
    "            ])\n",
    "\n",
    "\n",
    "augmentation = tio.RandomAffine(scales=(0.9, 1.1), degrees=(-10, 10))\n",
    "\n",
    "\n",
    "val_transform = process\n",
    "train_transform = tio.Compose([process, augmentation])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-construction",
   "metadata": {},
   "source": [
    "Define the train and validation dataset. We use 105 subjects for training and 13 for testing. <br />\n",
    "In order to help the segmentation network learn, we use the LabelSampler with p=0.2 for background, p=0.3 for liver and p=0.5 for liver tumors with a patch size of ($96 \\times 96 \\times 96$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ethical-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tio.SubjectsDataset(subjects[:105], transform=train_transform)\n",
    "val_dataset = tio.SubjectsDataset(subjects[105:], transform=val_transform)\n",
    "\n",
    "sampler = tio.data.LabelSampler(patch_size=96, label_name=\"Label\", label_probabilities={0:0.2, 1:0.3, 2:0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-smile",
   "metadata": {},
   "source": [
    "Create the queue to draw patches from.<br />\n",
    "The tio.Queue accepts a SubjectsDataset, a max_length argument describing the the number of patches that can be stored, the number of patches to draw from each subject, a sampler and the number of workers:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae28c9b6",
   "metadata": {},
   "source": [
    "## Data Loader Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "after-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_patches_queue = tio.Queue(\n",
    "     train_dataset,\n",
    "     max_length=40,\n",
    "     samples_per_volume=5,\n",
    "     sampler=sampler,\n",
    "     num_workers=4,\n",
    "    )\n",
    "\n",
    "val_patches_queue = tio.Queue(\n",
    "     val_dataset,\n",
    "     max_length=40,\n",
    "     samples_per_volume=5,\n",
    "     sampler=sampler,\n",
    "     num_workers=4,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-finding",
   "metadata": {},
   "source": [
    "Define train and val loader:\n",
    "\n",
    "NOTE: As the dataloaders only pop patches from the queue use 0 num workers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beneficial-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 # It will use nearly 30GB of ram in 16 Batch size. Select right batch_size for your device. You may overstressed your device.\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_patches_queue, batch_size=batch_size, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_patches_queue, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-midwest",
   "metadata": {},
   "source": [
    "We use the Adam optimizer with a learning rate of 1e-4 and a weighted cross-entropy loss, which assigns a threefold increased loss to tumorous voxels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0417fa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "third-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = UNet()   \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        pred = self.model(data)\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # You can obtain the raw volume arrays by accessing the data attribute of the subject\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        mask = batch[\"Label\"][\"data\"][:,0]  # Remove single channel as CrossEntropyLoss expects NxHxW\n",
    "        mask = mask.long()\n",
    "        \n",
    "        pred = self(img)\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Train Loss\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(img.cpu(), pred.cpu(), mask.cpu(), \"Train\")\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # You can obtain the raw volume arrays by accessing the data attribute of the subject\n",
    "        img = batch[\"CT\"][\"data\"]\n",
    "        mask = batch[\"Label\"][\"data\"][:,0]  # Remove single channel as CrossEntropyLoss expects NxHxW\n",
    "        mask = mask.long()\n",
    "        \n",
    "        pred = self(img)\n",
    "        loss = self.loss_fn(pred, mask)\n",
    "        \n",
    "        # Logs\n",
    "        self.log(\"Val Loss\", loss)\n",
    "        self.log_images(img.cpu(), pred.cpu(), mask.cpu(), \"Val\")\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def log_images(self, img, pred, mask, name):\n",
    "        \n",
    "        results = []\n",
    "        pred = torch.argmax(pred, 1) # Take the output with the highest value\n",
    "        axial_slice = 50  # Always plot slice 50 of the 96 slices\n",
    "        \n",
    "        fig, axis = plt.subplots(1, 2)\n",
    "        axis[0].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(mask[0][:,:,axial_slice]==0, mask[0][:,:,axial_slice])\n",
    "        axis[0].imshow(mask_, alpha=0.6)\n",
    "        axis[0].set_title(\"Ground Truth\")\n",
    "        \n",
    "        axis[1].imshow(img[0][0][:,:,axial_slice], cmap=\"bone\")\n",
    "        mask_ = np.ma.masked_where(pred[0][:,:,axial_slice]==0, pred[0][:,:,axial_slice])\n",
    "        axis[1].imshow(mask_, alpha=0.6, cmap=\"autumn\")\n",
    "        axis[1].set_title(\"Pred\")\n",
    "\n",
    "        self.logger.experiment.add_figure(f\"{name} Prediction vs Label\", fig, self.global_step)\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        #Caution! You always need to return a list here (just pack your optimizer into one :))\n",
    "        return [self.optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "first-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the model\n",
    "model = Segmenter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "lovely-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val Loss',\n",
    "    save_top_k=10,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cardiac-mouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furka\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:474: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Change the gpus parameter to the number of available gpus in your computer. Use 0 for CPU training\n",
    "gpus = 1 \n",
    "trainer = pl.Trainer(gpus=gpus, logger=TensorBoardLogger(save_dir=\"./logs\"), log_every_n_steps=1,\n",
    "                     callbacks=checkpoint_callback,\n",
    "                     max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "finite-superintendent",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3050 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | model   | UNet             | 5.8 M \n",
      "1 | loss_fn | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "5.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "5.8 M     Total params\n",
      "23.344    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furka\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\furka\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:84: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 2. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "C:\\Users\\furka\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e657455914d4648abd2f73a953098ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\furka\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# Train the model.\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
